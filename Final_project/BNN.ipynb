{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01c7758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data analysis and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualization settings\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "\n",
    "# Load the dataset\n",
    "#file_path = 'C:/Users/pc/Desktop/politecnico/b-FINTECH/business cases/Fintech_BC/BC4/data/Dataset4_EWS.xlsx'\n",
    "file_path = './data/Dataset4_EWS.xlsx'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24adc84b",
   "metadata": {},
   "source": [
    "## Real-World dataset\n",
    "\n",
    "From Bloomberg, consisting of weekly observations of:\n",
    "\n",
    "- Market and macroeconomic indicators (e.g., indices, rates).\n",
    "- A response variable `Y` indicating **anomalous periods** (e.g., market stress events)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5bad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set visualization style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "\n",
    "# Load the data from the Excel file\n",
    "# First sheet contains market data with dates and anomaly labels, second sheet contains metadata\n",
    "data_df = pd.read_excel(file_path, sheet_name='Markets')\n",
    "metadata_df = pd.read_excel(file_path, sheet_name='Metadata')\n",
    "\n",
    "# Check the structure of the loaded data\n",
    "print(\"Data columns:\", data_df.columns.tolist())\n",
    "\n",
    "# Extract date and anomaly label columns\n",
    "date_col = 'Date' if 'Date' in data_df.columns else data_df.columns[0]\n",
    "y_col = 'Y' if 'Y' in data_df.columns else None\n",
    "\n",
    "# Convert dates to datetime format\n",
    "data_df[date_col] = pd.to_datetime(data_df[date_col], dayfirst=True)  # Date format is dd/mm/yy\n",
    "\n",
    "# Set date as index\n",
    "data_df = data_df.set_index(date_col)\n",
    "\n",
    "# Extract features (all columns except Y if it exists)\n",
    "if y_col:\n",
    "    X_df = data_df.drop(y_col, axis=1)\n",
    "    y = data_df[y_col].values\n",
    "else:\n",
    "    X_df = data_df\n",
    "    y = None\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Data shape: {X_df.shape}\")\n",
    "print(f\"Total number of records: {len(X_df)}\")\n",
    "print(f\"Time period: from {X_df.index.min().strftime('%m/%d/%Y')} to {X_df.index.max().strftime('%m/%d/%Y')}\")\n",
    "print(f\"Frequency: {pd.infer_freq(X_df.index) or 'Weekly'}\")\n",
    "print(f\"Number of variables: {X_df.shape[1]}\")\n",
    "if y_col:\n",
    "    print(f\"Number of anomalies: {np.sum(y == 1)} ({np.mean(y == 1)*100:.2f}%)\")\n",
    "\n",
    "# Create a more comprehensive metadata table with additional statistics\n",
    "enhanced_metadata = []\n",
    "\n",
    "# Determine the correct column names for ticker and description\n",
    "ticker_col = 'ticker' if 'ticker' in metadata_df.columns else metadata_df.columns[0]\n",
    "desc_col = 'description' if 'description' in metadata_df.columns else metadata_df.columns[1] if len(metadata_df.columns) > 1 else ticker_col\n",
    "\n",
    "for ticker in X_df.columns:\n",
    "    # Get metadata for this ticker if available\n",
    "    meta_row = metadata_df[metadata_df[ticker_col] == ticker] if ticker in metadata_df[ticker_col].values else pd.DataFrame()\n",
    "\n",
    "    # Get description or use ticker if not found\n",
    "    description = meta_row[desc_col].values[0] if not meta_row.empty and desc_col in meta_row.columns else ticker\n",
    "\n",
    "    # Calculate statistics for this series\n",
    "    series = X_df[ticker]\n",
    "\n",
    "    enhanced_metadata.append({\n",
    "        'Ticker': ticker,\n",
    "        'Description': description,\n",
    "        'Mean': series.mean(),\n",
    "        'Std.Dev': series.std(),\n",
    "        'Min': series.min(),\n",
    "        'Max': series.max(),\n",
    "        'Missing values': series.isna().sum(),\n",
    "        'Missing (%)': f\"{series.isna().mean()*100:.2f}%\"\n",
    "    })\n",
    "\n",
    "# Create enhanced metadata dataframe\n",
    "enhanced_meta_df = pd.DataFrame(enhanced_metadata)\n",
    "\n",
    "# Display the enhanced metadata\n",
    "print(\"\\nMetadata and statistics:\")\n",
    "display(enhanced_meta_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc49041",
   "metadata": {},
   "source": [
    "# Mixture model of Gaussian distributions\n",
    "- The data is assumed to be generated from a mixture of Gaussian distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe01dac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Make data stationary based on variable type\n",
    "# So w define lists of variables by type\n",
    "indices_currencies = [col for col in X_df.columns if col in [\n",
    "    'XAUBGNL', 'BDIY', 'CRY', 'Cl1', 'DXY', 'EMUSTRUU', 'GBP', 'JPY', 'LF94TRUU',\n",
    "    'LF98TRUU', 'LG30TRUU', 'LMBITR', 'LP01TREU', 'LUACTRUU', 'LUMSTRUU',\n",
    "    'MXBR', 'MXCN', 'MXEU', 'MXIN', 'MXJP', 'MXRU', 'MXUS', 'VIX'\n",
    "]]\n",
    "\n",
    "interest_rates = [col for col in X_df.columns if col in [\n",
    "    'EONIA', 'GTDEM10Y', 'GTDEM2Y', 'GTDEM30Y', 'GTGBP20Y', 'GTGBP2Y', 'GTGBP30Y',\n",
    "    'GTITL10YR', 'GTITL2YR', 'GTITL30YR', 'GTJPY10YR', 'GTJPY2YR', 'GTJPY30YR',\n",
    "    'US0001M', 'USGG3M', 'USGG2YR', 'GT10', 'USGG30YR'\n",
    "]]\n",
    "\n",
    "# Create a new dataframe for stationary data\n",
    "stationary_df = pd.DataFrame(index=X_df.index[1:])\n",
    "\n",
    "# Apply log-differences to indices and currencies (always positive)\n",
    "for col in indices_currencies:\n",
    "    if col in X_df.columns:\n",
    "        stationary_df[col] = np.diff(np.log(X_df[col]))\n",
    "\n",
    "# Apply first differences to interest rates (can be negative or very close to 0)\n",
    "for col in interest_rates:\n",
    "    if col in X_df.columns:\n",
    "        stationary_df[col] = np.diff(X_df[col])\n",
    "\n",
    "# Keep Bloomberg Economic US Surprise Index as is (already stationary)\n",
    "if 'ECSURPUS' in X_df.columns:\n",
    "    stationary_df['ECSURPUS'] = X_df['ECSURPUS'].values[1:]\n",
    "\n",
    "# Adjust the response variable to match the new data length\n",
    "if y is not None:\n",
    "    y_stationary = y[1:]\n",
    "else:\n",
    "    y_stationary = None\n",
    "\n",
    "# Convert to numpy arrays for easier manipulation\n",
    "X = stationary_df.values\n",
    "y = y_stationary\n",
    "\n",
    "# Reshuffle the data (this will break down autocorrelation)\n",
    "X_shuffled, y_shuffled = shuffle(X, y, random_state=42)\n",
    "\n",
    "# Separate normal and anomalous examples\n",
    "X_normal = X_shuffled[y_shuffled == 0]\n",
    "X_anomaly = X_shuffled[y_shuffled == 1]\n",
    "\n",
    "# Training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_shuffled, y_shuffled, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64981ff",
   "metadata": {},
   "source": [
    "# Plot differences \n",
    "- The data is plotted to visualize the differences between the two classes.\n",
    "\n",
    "As we can see there isn't a clear separation between the two classes differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d90c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 40))\n",
    "# Calculate the number of rows and columns for the subplot grid\n",
    "num_cols = 4\n",
    "num_rows = -(-len(stationary_df.columns) // num_cols)  # Ceiling division\n",
    "\n",
    "# For each column in the stationary dataframe, plot separate histograms for y=0 and y=1\n",
    "for i, col in enumerate(stationary_df.columns):\n",
    "    if col != 'y':  # Skip the target column itself\n",
    "        plt.subplot(num_rows, num_cols, i + 1)\n",
    "        \n",
    "        # Class 0 (blue)\n",
    "        plt.hist(stationary_df[y_stationary == 0][col].dropna(), \n",
    "                 bins=30, color='blue', alpha=0.5, label='y=0')\n",
    "        \n",
    "        # Class 1 (red)\n",
    "        plt.hist(stationary_df[y_stationary == 1][col].dropna(), \n",
    "                 bins=30, color='red', alpha=0.5, label='y=1')\n",
    "        \n",
    "        plt.title(col)\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a22d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_df shape\", X_df.shape)\n",
    "\n",
    "plt.figure(figsize=(20, 40))\n",
    "# Calculate the number of rows and columns for the subplot grid\n",
    "num_cols = 4\n",
    "num_rows = -(-len(X_df.columns) // num_cols)  # Ceiling division\n",
    "y = data_df[y_col].values\n",
    "X_df = data_df.drop(y_col, axis=1)\n",
    "\n",
    "\n",
    "# For each column in the stationary dataframe, plot separate histograms for y=0 and y=1\n",
    "for i, col in enumerate(X_df.columns):\n",
    "    if col != 'y':  # Skip the target column itself\n",
    "        plt.subplot(num_rows, num_cols, i + 1)\n",
    "        \n",
    "        # Class 0 (blue)\n",
    "        plt.hist(X_df[y == 0][col].dropna(), \n",
    "                 bins=30, color='blue', alpha=0.5, label='y=0')\n",
    "        \n",
    "        # Class 1 (red)\n",
    "        plt.hist(X_df[y == 1][col].dropna(), \n",
    "                 bins=30, color='red', alpha=0.5, label='y=1')\n",
    "        \n",
    "        plt.title(col)\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f514ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "class BayesianLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout_prob=0.1, activation='gelu', use_batch_norm=True):\n",
    "        super(BayesianLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        \n",
    "        if use_batch_norm:\n",
    "            self.batch_norm = nn.BatchNorm1d(out_features)\n",
    "        \n",
    "        # Activation function selection\n",
    "        if activation == 'relu':\n",
    "            self.activation = F.relu\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.activation = F.leaky_relu\n",
    "        elif activation == 'selu':\n",
    "            self.activation = F.selu\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = F.gelu\n",
    "        elif activation == 'swish':\n",
    "            self.activation = lambda x: x * torch.sigmoid(x)\n",
    "        else:\n",
    "            self.activation = F.gelu  # Default \n",
    "            \n",
    "        # Initialize weights with He initialization for better gradient flow\n",
    "        nn.init.kaiming_uniform_(self.linear.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        if self.use_batch_norm:\n",
    "            if len(x.shape) == 2:\n",
    "                x = self.batch_norm(x)\n",
    "            else:\n",
    "                # Handle case when batch size is 1 during inference\n",
    "                x = x.unsqueeze(0) if len(x.shape) == 1 else x\n",
    "                x = self.batch_norm(x)\n",
    "                x = x.squeeze(0) if len(x.shape) == 3 else x\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# Residual block for better gradient flow\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, features, dropout_prob=0.1, activation='gelu', use_batch_norm=True):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.layer1 = BayesianLayer(features, features, dropout_prob, activation, use_batch_norm)\n",
    "        self.layer2 = BayesianLayer(features, features, dropout_prob, activation, use_batch_norm)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out += residual  # Skip connection\n",
    "        return out\n",
    "\n",
    "# Improved Bayesian Neural Network with Residual Connections\n",
    "class BayesianNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[32, 64, 64, 32], dropout_probs=[0.1, 0.2, 0.2, 0.1], \n",
    "                 activation='gelu', use_batch_norm=True, use_residual=True):\n",
    "        super(BayesianNN, self).__init__()\n",
    "        \n",
    "        self.use_residual = use_residual\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers.append(BayesianLayer(input_dim, hidden_dims[0], \n",
    "                                        dropout_probs[0], activation, use_batch_norm))\n",
    "        \n",
    "        # Hidden layers with potential residual connections\n",
    "        for i in range(1, len(hidden_dims)):\n",
    "            if use_residual and hidden_dims[i] == hidden_dims[i-1]:\n",
    "                self.layers.append(ResidualBlock(hidden_dims[i-1], dropout_probs[i], \n",
    "                                                activation, use_batch_norm))\n",
    "            else:\n",
    "                self.layers.append(BayesianLayer(hidden_dims[i-1], hidden_dims[i], \n",
    "                                                dropout_probs[i], activation, use_batch_norm))\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = torch.sigmoid(self.output_layer(x))\n",
    "        return x\n",
    "    \n",
    "    def predict_with_uncertainty(self, x, num_samples=100):\n",
    "        # Enable dropout for prediction to simulate MC Dropout\n",
    "        self.train()\n",
    "        \n",
    "        predictions = []\n",
    "        for _ in range(num_samples):\n",
    "            with torch.no_grad():\n",
    "                pred = self(x)\n",
    "                predictions.append(pred)\n",
    "        \n",
    "        # Stack all predictions\n",
    "        stacked_preds = torch.stack(predictions, dim=1)\n",
    "        \n",
    "        # Calculate mean and standard deviation over samples\n",
    "        mean_pred = torch.mean(stacked_preds, dim=1, keepdim=True)\n",
    "        std_pred = torch.std(stacked_preds, dim=1, keepdim=True)\n",
    "        \n",
    "        return mean_pred, std_pred, stacked_preds\n",
    "\n",
    "# Learning rate scheduler\n",
    "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_epochs, max_epochs, eta_min=0, last_epoch=-1):\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.max_epochs = max_epochs\n",
    "        self.eta_min = eta_min\n",
    "        super(CosineWarmupScheduler, self).__init__(optimizer, last_epoch)\n",
    "        \n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.warmup_epochs:\n",
    "            # Linear warmup\n",
    "            alpha = self.last_epoch / self.warmup_epochs\n",
    "            return [base_lr * alpha for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            # Cosine annealing\n",
    "            progress = (self.last_epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs)\n",
    "            cosine_decay = 0.5 * (1 + np.cos(np.pi * progress))\n",
    "            return [self.eta_min + (base_lr - self.eta_min) * cosine_decay for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab5740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_improved_bnn(X, y, hidden_dims=[32, 64, 64, 32],\n",
    "                                  dropout_probs=[0.1, 0.2, 0.2, 0.1], \n",
    "                                  activation='gelu', use_batch_norm=True, use_residual=True,\n",
    "                                  epochs=300, batch_size=32, lr=0.001, weight_decay=1e-4,\n",
    "                                  warmup_epochs=30, class_weights=None):\n",
    "    \"\"\"\n",
    "    Train and evaluate an Improved Bayesian Neural Network\n",
    "    \"\"\"\n",
    "    # Ensure y is binary and has the right format\n",
    "    y = y.astype(int)\n",
    "    \n",
    "    # Split data - stratify to handle class imbalance\n",
    "    X_train, X__val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X__val_test, y_val_test, test_size=0.5, random_state=42, stratify=y_val_test)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).reshape(-1, 1)\n",
    "    X_val_tensor = torch.FloatTensor(X_val)\n",
    "    y_val_tensor = torch.FloatTensor(y_val).reshape(-1, 1)\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_test_tensor = torch.FloatTensor(y_test).reshape(-1, 1)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    \n",
    "    # Initialize model\n",
    "    input_dim = X_train.shape[1]\n",
    "    bnn_model = BayesianNN(input_dim, hidden_dims, dropout_probs, \n",
    "                                  activation, use_batch_norm, use_residual)\n",
    "\n",
    "    # Define loss function with class weights if provided\n",
    "    if class_weights is not None:\n",
    "        # Calculate class weights if not provided\n",
    "        if class_weights == 'auto':\n",
    "            counts = np.bincount(y_train.astype(int))\n",
    "            class_weights = torch.FloatTensor([1.0, counts[0] / counts[1]])  # Inverse frequency\n",
    "        else:\n",
    "            class_weights = torch.FloatTensor(class_weights)\n",
    "        criterion = nn.BCELoss(weight=None)  # Will apply weights manually\n",
    "    else:\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "    # Define optimizer with weight decay (L2 regularization)\n",
    "    optimizer = optim.AdamW(bnn_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = CosineWarmupScheduler(optimizer, warmup_epochs, epochs, eta_min=lr/10)\n",
    "\n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    min_val_loss = float('inf')\n",
    "    patience = 0.3 * epochs  # Early stopping patience\n",
    "    patience_counter = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        bnn_model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = bnn_model(batch_X)\n",
    "            \n",
    "            # Apply class weights if needed\n",
    "            if class_weights is not None:\n",
    "                batch_weights = torch.ones_like(batch_y)\n",
    "                for i in range(len(class_weights)):\n",
    "                    batch_weights[batch_y == i] = class_weights[i]\n",
    "                loss = F.binary_cross_entropy(outputs, batch_y, weight=batch_weights)\n",
    "            else:\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(bnn_model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track losses\n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "        # Validation step\n",
    "        bnn_model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                outputs = bnn_model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "            val_losses.append(val_loss / len(val_loader))\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            # Save the best model\n",
    "            best_model = bnn_model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Print progress every 50 epochs\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "\n",
    "        # Restore the best model\n",
    "        if best_model is not None:\n",
    "            bnn_model.load_state_dict(best_model)\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training loss')\n",
    "    plt.plot(val_losses, label='Validation loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot learning rate\n",
    "    lr_history = []\n",
    "    dummy_scheduler = CosineWarmupScheduler(optimizer, warmup_epochs, epochs, eta_min=lr/10)\n",
    "    for i in range(epochs):\n",
    "        lr_history.append(dummy_scheduler.get_lr()[0])\n",
    "        dummy_scheduler.step()\n",
    "        \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(lr_history)\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Make predictions with uncertainty\n",
    "    bnn_model.eval()  # This won't disable dropout for uncertainty estimation\n",
    "    mean_pred, std_pred, all_pred_samples = bnn_model.predict_with_uncertainty(X_test_tensor)\n",
    "\n",
    "    # Convert to numpy\n",
    "    mean_pred_np = mean_pred.numpy()\n",
    "    std_pred_np = std_pred.numpy()\n",
    "    all_samples_np = all_pred_samples.numpy()\n",
    "\n",
    "    # Calculate optimal threshold for F1 score (instead of default 0.5)\n",
    "    thresholds = np.linspace(0.1, 0.9, 100)\n",
    "    f1_scores = []\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (mean_pred_np > threshold).astype(int).flatten()\n",
    "        f1 = f1_score(y_test.flatten(), y_pred)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "    print(f\"Optimal threshold for F1 score: {optimal_threshold:.3f}\")\n",
    "    \n",
    "    # Convert probabilities to class labels with optimal threshold\n",
    "    y_pred_class = (mean_pred_np > optimal_threshold).astype(int).flatten()\n",
    "    \n",
    "    # Ensure y_test is flattened as well for proper comparison\n",
    "    y_test_flat = y_test.flatten()\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test_flat, y_pred_class)\n",
    "    prec = precision_score(y_test_flat, y_pred_class)\n",
    "    rec = recall_score(y_test_flat, y_pred_class)\n",
    "    f1 = f1_score(y_test_flat, y_pred_class)\n",
    "    conf_matrix = confusion_matrix(y_test_flat, y_pred_class)\n",
    "    \n",
    "    print(f\"Test accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Confusion Matrix: \\n{conf_matrix}\")\n",
    "\n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test_flat, mean_pred_np.flatten())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # ROC curve\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.3f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # F1 Score vs Threshold\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(thresholds, f1_scores)\n",
    "    plt.axvline(x=optimal_threshold, color='r', linestyle='--', label=f'Optimal = {optimal_threshold:.2f}')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('F1 Score vs Threshold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Uncertainty vs. error correlation\n",
    "    errors = np.abs(y_test_flat - mean_pred_np.flatten())\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.scatter(std_pred_np.flatten(), errors, alpha=0.5)\n",
    "    plt.xlabel('Predictive Uncertainty (std)')\n",
    "    plt.ylabel('Absolute Error')\n",
    "    plt.title('Uncertainty vs Error Correlation')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add trend line\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(std_pred_np.flatten(), errors)\n",
    "    x = np.linspace(min(std_pred_np.flatten()), max(std_pred_np.flatten()), 100)\n",
    "    y = slope * x + intercept\n",
    "    plt.plot(x, y, 'r-', label=f'R² = {r_value**2:.3f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calibration plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # Group predictions into bins\n",
    "    n_bins = 10\n",
    "    bin_edges = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_indices = np.digitize(mean_pred_np.flatten(), bin_edges) - 1\n",
    "    bin_indices = np.clip(bin_indices, 0, n_bins - 1)\n",
    "    \n",
    "    # Calculate mean predicted probability and actual frequency for each bin\n",
    "    bin_probs = np.zeros(n_bins)\n",
    "    bin_freqs = np.zeros(n_bins)\n",
    "    bin_counts = np.zeros(n_bins)\n",
    "    \n",
    "    for i in range(len(y_test_flat)):\n",
    "        bin_probs[bin_indices[i]] += mean_pred_np.flatten()[i]\n",
    "        bin_freqs[bin_indices[i]] += y_test_flat[i]\n",
    "        bin_counts[bin_indices[i]] += 1\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    valid_bins = bin_counts > 0\n",
    "    bin_probs[valid_bins] /= bin_counts[valid_bins]\n",
    "    bin_freqs[valid_bins] /= bin_counts[valid_bins]\n",
    "    \n",
    "    # Plot calibration curve\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    plt.plot(bin_centers[valid_bins], bin_freqs[valid_bins], 'o-', label='Model')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')\n",
    "    plt.xlabel('Mean predicted probability')\n",
    "    plt.ylabel('Actual frequency')\n",
    "    plt.title('Calibration Plot')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return bnn_model, scaler, optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e8dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_df.drop(y_col, axis=1).values\n",
    "y = data_df[y_col].values\n",
    "\n",
    "model, scaler, threshold = train_and_evaluate_improved_bnn(\n",
    "    X, y,\n",
    "    hidden_dims=[32, 128, 256, 128, 32],  # Deeper network with varying widths\n",
    "    dropout_probs=[0.1, 0.2, 0.2, 0.2, 0.1],  # Varying dropout rates\n",
    "    activation='gelu',  # Modern activation function\n",
    "    use_batch_norm=True,  # Use batch normalization\n",
    "    use_residual=True,  # Use residual connections\n",
    "    epochs=1000,\n",
    "    batch_size=32,\n",
    "    lr=0.001,\n",
    "    weight_decay=1e-4,  # L2 regularization\n",
    "    warmup_epochs=100,\n",
    "    class_weights='auto'  # Automatically handle class imbalance\n",
    ")\n",
    "\n",
    "print(\"BNN training and evaluation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
