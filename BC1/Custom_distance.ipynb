{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Distance \n",
    "\n",
    "This notebook contains all the experiments done with the custom distance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup locale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path su linux\n",
    "path = './data/Dataset1_BankClients.xlsx'\n",
    "\n",
    "# Path su windows - comando da inserire \n",
    "#path = ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_optimal_k_voting(X, cluster_results, custom_distances, verbose = False):\n",
    "    \"\"\"\n",
    "    Determine the optimal number of clusters (k) by voting among different metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Dataset (used for Calinski-Harabasz and Davies-Bouldin scores)\n",
    "    - cluster_results: Dictionary where keys are k values and values are cluster assignments\n",
    "    - custom_distances: Precomputed costum distance matrix (used for Silhouette score)\n",
    "    - print: Whether to print some log \n",
    "    \n",
    "    Returns:\n",
    "    - optimal_k: The optimal number of clusters based on the median of suggested k values\n",
    "    \"\"\"\n",
    "    \n",
    "    if not cluster_results:\n",
    "        raise ValueError(\"cluster_results cannot be empty\")\n",
    "    \n",
    "    k_values = list(cluster_results.keys())\n",
    "    \n",
    "    # Validate cluster assignments\n",
    "    for k, assignments in cluster_results.items():\n",
    "        if len(assignments) != len(X):\n",
    "            raise ValueError(f\"Cluster assignments for k={k} do not match dataset size\")\n",
    "    \n",
    "    # Get optimal k for each metric\n",
    "    try:\n",
    "        ch_optimal = k_values[np.argmax(calinski_harabasz_score(X, cluster_results[k]) for k in k_values)]\n",
    "        # print(\"DEBUG: \", ch_optimal)\n",
    "        # print(\"Calinski-Harabasz Score (k=3):\", calinski_harabasz_score(X, cluster_results[3]))\n",
    "        # print(\"Calinski-Harabasz Score (k=4):\", calinski_harabasz_score(X, cluster_results[4]))\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning: Calinski-Harabasz score failed. Skipping. Error: {e}\")\n",
    "        ch_optimal = None\n",
    "    \n",
    "    try:\n",
    "        db_scores = {k: davies_bouldin_score(X, cluster_results[k]) for k in k_values}\n",
    "        db_optimal = min(db_scores, key=db_scores.get)\n",
    "        \n",
    "        # print(\"DEBUG: \", db_optimal)\n",
    "        # print(\"Davies-Bouldin Score (k=3):\", db_scores[3])\n",
    "        # print(\"Davies-Bouldin Score (k=4):\", db_scores[4])\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning: Davies-Bouldin score failed. Skipping. Error: {e}\")\n",
    "        db_optimal = None\n",
    "\n",
    "    \n",
    "    try:\n",
    "        # Correctly identify the optimal k based on the Silhouette score\n",
    "        sil_scores = {k: silhouette_score(custom_distances, cluster_results[k], metric='precomputed') for k in k_values}\n",
    "        sil_optimal = max(sil_scores, key=sil_scores.get)\n",
    "        \n",
    "        # print(\"DEBUG: \", sil_optimal)\n",
    "        # print(\"Silhouette Score (k=3):\", sil_scores[3])\n",
    "        # print(\"Silhouette Score (k=4):\", sil_scores[4])\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning: Silhouette score failed. Skipping. Error: {e}\")\n",
    "        sil_optimal = None\n",
    "    \n",
    "    # Collect valid suggestions\n",
    "    suggestions = [k for k in [ch_optimal, db_optimal, sil_optimal] if k is not None]\n",
    "    print(\"Suggestions:\", suggestions)\n",
    "    \n",
    "    if not suggestions:\n",
    "        raise ValueError(\"No valid suggestions from metrics\")\n",
    "    \n",
    "    # Get median k\n",
    "    optimal_k = int(np.median(suggestions))\n",
    "    \n",
    "    # Print results\n",
    "    if(verbose):\n",
    "        print(\"\\nVoting Results:\")\n",
    "        if ch_optimal is not None:\n",
    "            print(f\"Calinski-Harabasz score: {calinski_harabasz_score(X, cluster_results[ch_optimal])}\")\n",
    "        if db_optimal is not None:\n",
    "            print(f\"Davies-Bouldin score: {davies_bouldin_score(X, cluster_results[db_optimal])}\")\n",
    "        if sil_optimal is not None:\n",
    "            print(f\"Silhouette score: {silhouette_score(custom_distances, cluster_results[sil_optimal], metric='precomputed')}\")\n",
    "        print(f\"\\nFinal decision (median): {optimal_k} clusters\")\n",
    "    \n",
    "    return optimal_k\n",
    "\n",
    "def test_different_cluster_kmedoids(X, custom_dist, k_values):\n",
    "    \"\"\"\n",
    "    Test different number of clusters using K-Medoids clustering algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Dataset\n",
    "    - custom_dist: Precomputed distance matrix\n",
    "    - k_values: List of k values to test\n",
    "\n",
    "    Returns:\n",
    "    - cluster_results: Dictionary where keys are k values and values are cluster assignments\n",
    "    - silhouette_score: Silhouette score for the optimal k\n",
    "    \"\"\"\n",
    "    cluster_results = {}\n",
    "    for i in range(len(k_values)):\n",
    "        k = k_values[i]\n",
    "        kmedoids = KMedoids(n_clusters=k, random_state=42)\n",
    "        kmedoids.fit(custom_dist)\n",
    "        cluster_results[k] = kmedoids.predict(custom_dist)\n",
    "    \n",
    "    final_k = get_optimal_k_voting(X, cluster_results, custom_dist)\n",
    "\n",
    "    return final_k, silhouette_score(custom_dist, cluster_results[final_k], metric='precomputed')\n",
    "\n",
    "def test_different_cluster_kmeans(X, k_values):\n",
    "    \"\"\"\n",
    "    Test different number of clusters using K-Means clustering algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Dataset\n",
    "    - k_values: List of k values to test\n",
    "\n",
    "    Returns:\n",
    "    - final_k: \n",
    "    - silhouette_score: Silhouette score for the optimal k\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    cluster_results = {}\n",
    "    for i in range(len(k_values)):\n",
    "        k = k_values[i]\n",
    "        kmeans = KMeans(n_clusters=k, metric='precomputed', random_state=42)\n",
    "        kmeans.fit(X)\n",
    "        cluster_results[k] = kmeans.predict(X)\n",
    "    \n",
    "    final_k = get_optimal_k_voting(X, cluster_results, X)\n",
    "\n",
    "    return final_k, silhouette_score(X, cluster_results[final_k], metric='precomputed')\n",
    "\n",
    "def visualize_cluster(X, clusters, custom_distance):\n",
    "    \"\"\"\n",
    "    Visualize the clustering results in 2D and 3D space using PCA and ICA.\n",
    "    Parameters:\n",
    "    - X: Dataset\n",
    "    - clusters: Cluster assignments\n",
    "    - custom_distance: Precomputed distance matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(n_components=3)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    \n",
    "    # Create subplots for PCA visualization\n",
    "    fig_pca = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        specs=[[{\"type\": \"xy\"}, {\"type\": \"scene\"}]],\n",
    "        horizontal_spacing=0.15,\n",
    "        subplot_titles=[\"2-D Embedding with PCA\", \"3-D Embedding with PCA\"]\n",
    "    )\n",
    "    \n",
    "    # Add points for each cluster\n",
    "    unique_clusters = np.unique(clusters)\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'cyan', 'magenta', 'yellow']\n",
    "    \n",
    "    for cluster in unique_clusters:\n",
    "        # Get points in current cluster\n",
    "        cluster_points = X_pca[clusters == cluster]\n",
    "        \n",
    "        # Select color for the cluster\n",
    "        cluster_color = colors[cluster % len(colors)]\n",
    "        \n",
    "        # Add points for 2D scatter\n",
    "        fig_pca.add_trace(\n",
    "            go.Scatter(\n",
    "                x=cluster_points[:, 0],\n",
    "                y=cluster_points[:, 1],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    color=cluster_color,\n",
    "                    opacity=0.8,\n",
    "                    size=8\n",
    "                ),\n",
    "                name=f'Cluster {cluster} Points',\n",
    "                showlegend=True,\n",
    "                legendgroup=f'group{cluster}',\n",
    "                xaxis='x',\n",
    "                yaxis='y'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Add points for 3D scatter\n",
    "        fig_pca.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=cluster_points[:, 0],\n",
    "                y=cluster_points[:, 1],\n",
    "                z=cluster_points[:, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    color=cluster_color,\n",
    "                    opacity=0.8,\n",
    "                    size=5\n",
    "                ),\n",
    "                name=f'Cluster {cluster} Points 3D',\n",
    "                showlegend=True,\n",
    "                legendgroup=f'group{cluster}'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Update 2D layout\n",
    "    fig_pca.update_xaxes(title_text=\"Principal Component 1\")\n",
    "    fig_pca.update_yaxes(title_text=\"Principal Component 2\")\n",
    "    \n",
    "    # Update 3D layout\n",
    "    fig_pca.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title=\"Principal Component 1\",\n",
    "            yaxis_title=\"Principal Component 2\",\n",
    "            zaxis_title=\"Principal Component 3\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    ################# ---------------------- ICA ---------------------- #################\n",
    "    ica = FastICA(n_components=3, random_state=42)\n",
    "    X_ica = ica.fit_transform(X)\n",
    "    \n",
    "    # Create subplots for ICA visualization\n",
    "    fig_ica = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        specs=[[{\"type\": \"xy\"}, {\"type\": \"scene\"}]],\n",
    "        horizontal_spacing=0.15,\n",
    "        subplot_titles=[\"2-D Embedding with ICA\", \"3-D Embedding with ICA\"]\n",
    "    )\n",
    "    \n",
    "    # Add points for each cluster\n",
    "    for cluster in unique_clusters:\n",
    "        # Get points in current cluster\n",
    "        cluster_points = X_ica[clusters == cluster]\n",
    "        \n",
    "        # Select color for the cluster\n",
    "        cluster_color = colors[cluster % len(colors)]\n",
    "        \n",
    "        # Add points for 2D scatter\n",
    "        fig_ica.add_trace(\n",
    "            go.Scatter(\n",
    "                x=cluster_points[:, 0],\n",
    "                y=cluster_points[:, 1],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    color=cluster_color,\n",
    "                    opacity=0.8,\n",
    "                    size=8\n",
    "                ),\n",
    "                name=f'Cluster {cluster} Points',\n",
    "                showlegend=True,\n",
    "                legendgroup=f'group{cluster}',\n",
    "                xaxis='x',\n",
    "                yaxis='y'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Add points for 3D scatter\n",
    "        fig_ica.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=cluster_points[:, 0],\n",
    "                y=cluster_points[:, 1],\n",
    "                z=cluster_points[:, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    color=cluster_color,\n",
    "                    opacity=0.8,\n",
    "                    size=5\n",
    "                ),\n",
    "                name=f'Cluster {cluster} Points 3D',\n",
    "                showlegend=True,\n",
    "                legendgroup=f'group{cluster}'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Update 2D layout\n",
    "    fig_ica.update_xaxes(title_text=\"Independent Component 1\")\n",
    "    fig_ica.update_yaxes(title_text=\"Independent Component 2\")\n",
    "    \n",
    "    # Update 3D layout\n",
    "    fig_ica.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title=\"Independent Component 1\",\n",
    "            yaxis_title=\"Independent Component 2\",\n",
    "            zaxis_title=\"Independent Component 3\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    ################# ---------------------- t-SNE ---------------------- #################\n",
    "    # 2D t-SNE\n",
    "    tsne_2d = TSNE(n_components=2, random_state=42, perplexity=30, metric='precomputed', init='random')\n",
    "    X_tsne_2d = tsne_2d.fit_transform(custom_distance)\n",
    "    \n",
    "    # 3D t-SNE\n",
    "    tsne_3d = TSNE(n_components=3, random_state=42, perplexity=30, metric='precomputed', init='random')\n",
    "    X_tsne_3d = tsne_3d.fit_transform(custom_distance)\n",
    "    \n",
    "    # Create subplots for t-SNE visualization\n",
    "    fig_tsne = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        specs=[[{\"type\": \"xy\"}, {\"type\": \"scene\"}]],\n",
    "        horizontal_spacing=0.15,\n",
    "        subplot_titles=[\"2-D Embedding with t-SNE\", \"3-D Embedding with t-SNE\"]\n",
    "    )\n",
    "    \n",
    "    # Add points for each cluster\n",
    "    for cluster in unique_clusters:\n",
    "        # Get points in current cluster for 2D t-SNE\n",
    "        cluster_points_2d = X_tsne_2d[clusters == cluster]\n",
    "        # Get points in current cluster for 3D t-SNE\n",
    "        cluster_points_3d = X_tsne_3d[clusters == cluster]\n",
    "        \n",
    "        # Select color for the cluster\n",
    "        cluster_color = colors[cluster % len(colors)]\n",
    "        \n",
    "        # Add points for 2D scatter\n",
    "        fig_tsne.add_trace(\n",
    "            go.Scatter(\n",
    "                x=cluster_points_2d[:, 0],\n",
    "                y=cluster_points_2d[:, 1],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    color=cluster_color,\n",
    "                    opacity=0.8,\n",
    "                    size=8\n",
    "                ),\n",
    "                name=f'Cluster {cluster} Points',\n",
    "                showlegend=True,\n",
    "                legendgroup=f'group{cluster}',\n",
    "                xaxis='x',\n",
    "                yaxis='y'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Add points for 3D scatter\n",
    "        fig_tsne.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=cluster_points_3d[:, 0],\n",
    "                y=cluster_points_3d[:, 1],\n",
    "                z=cluster_points_3d[:, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    color=cluster_color,\n",
    "                    opacity=0.8,\n",
    "                    size=5\n",
    "                ),\n",
    "                name=f'Cluster {cluster} Points 3D',\n",
    "                showlegend=True,\n",
    "                legendgroup=f'group{cluster}'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Update 2D layout\n",
    "    fig_tsne.update_xaxes(title_text=\"t-SNE Component 1\")\n",
    "    fig_tsne.update_yaxes(title_text=\"t-SNE Component 2\")\n",
    "    \n",
    "    # Update 3D layout\n",
    "    fig_tsne.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title=\"t-SNE Component 1\",\n",
    "            yaxis_title=\"t-SNE Component 2\",\n",
    "            zaxis_title=\"t-SNE Component 3\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Show all visualizations\n",
    "    fig_pca.show()\n",
    "    fig_ica.show()\n",
    "    fig_tsne.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data in a DataFrame\n",
    "data = pd.read_excel(path)\n",
    "\n",
    "# Let's inspect the first 5 record\n",
    "print(data.head())\n",
    "# Drop the column by its actual name (e.g., 'ID' or the actual name of the column)\n",
    "data = data.drop(columns=['ID'])  # Replace 'ID' with the actual column name to drop\n",
    "\n",
    "# Test without esg and bankfriend\n",
    "#data = data.drop(columns=['ESG', 'BankFriend'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Specify categorical variables\n",
    "categorical_columns = ['Gender', 'Job', 'Area', 'CitySize', 'Investments']\n",
    "#categorical_columns = ['Gender', 'Job', 'Area', 'CitySize']\n",
    "\n",
    "\n",
    "# Split variables\n",
    "numerical_features = data.drop(columns=categorical_columns)  # Exclude categorical variables\n",
    "categorical_features = data[categorical_columns]  # Select categorical variables\n",
    "\n",
    "# Convert categorical in typ 'category' (for OneHotEncoder)\n",
    "categorical_features = categorical_features.astype('category')\n",
    "\n",
    "# Normalize numerical variables\n",
    "scaler = MinMaxScaler()\n",
    "X_num = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# One-hot encoding categorical variables\n",
    "encoder = OneHotEncoder(drop='first', handle_unknown='ignore')  # Dummy encoding - set 1 feature to all 0s - ignoring unknwown values\n",
    "X_cat = encoder.fit_transform(categorical_features).toarray()  # Convert into a dense matrix\n",
    "\n",
    "# Concatenation of numerical and categorical variables\n",
    "X = np.hstack((X_num, X_cat))\n",
    "\n",
    "# Summary of the dataset\n",
    "print(\"Numerical Features Shape:\", X_num.shape)\n",
    "print(\"Categorical Features Shape:\", X_cat.shape)\n",
    "print(\"Combined Features Shape:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist, mahalanobis\n",
    "from scipy.linalg import inv\n",
    "\n",
    "def vectorized_mahalanobis(X_num, inv_cov_matrix):\n",
    "    \"\"\"\n",
    "    Vectorized Mahalanobis distance calculation\n",
    "    Returns square matrix of pairwise distances\n",
    "    \"\"\"\n",
    "    # Compute differences between all pairs of points\n",
    "    diff = X_num[:, np.newaxis] - X_num[np.newaxis, :]\n",
    "    \n",
    "    # Matrix multiplication approach for Mahalanobis distance\n",
    "    return np.sqrt(np.sum(diff @ inv_cov_matrix * diff, axis=-1))\n",
    "\n",
    "def vectorized_euclidean(X_num):\n",
    "    \"\"\"\n",
    "    Vectorized Mahalanobis distance calculation\n",
    "    Returns square matrix of pairwise distances\n",
    "    \"\"\"\n",
    "    # Compute differences between all pairs of points\n",
    "    diff = X_num[:, np.newaxis] - X_num[np.newaxis, :]\n",
    "    \n",
    "    # Matrix multiplication approach for Mahalanobis distance\n",
    "    return np.sqrt(np.sum(diff * diff, axis=-1))\n",
    "\n",
    "def vectorized_overlap(X_cat):\n",
    "    \"\"\"\n",
    "    Vectorized overlap distance calculation\n",
    "    Returns square matrix of pairwise distances\n",
    "    \"\"\"\n",
    "    # Compute minimum between all pairs of points\n",
    "    min_size = np.minimum(\n",
    "        X_cat.sum(axis=1)[:, np.newaxis],\n",
    "        X_cat.sum(axis=1)[np.newaxis, :]\n",
    "    )\n",
    "    \n",
    "    # Compute intersection sum for all pairs\n",
    "    intersection = np.minimum(X_cat[:, np.newaxis], X_cat[np.newaxis, :]).sum(axis=-1)\n",
    "    \n",
    "    # Handle division by zero\n",
    "    mask = min_size > 0\n",
    "    result = np.ones_like(min_size, dtype=float)\n",
    "    result[mask] = 1 - intersection[mask] / min_size[mask]\n",
    "\n",
    "    # Set diagonal to zero\n",
    "    np.fill_diagonal(result, 0)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# # Drop age to numerical features\n",
    "# X_num = np.delete(X_num, 0, axis=1)\n",
    "\n",
    "# # Discretize Age \n",
    "# age = data['Age']\n",
    "# age_bins = [0, 35, 65]\n",
    "# age_labels = [0, 1, 2]\n",
    "# # full of zero \n",
    "# age_discretized = np.zeros((len(age), 1))\n",
    "# for i in range(len(age)):\n",
    "#     for j in range(len(age_bins)):\n",
    "#         if age[i] >= age_bins[j]:\n",
    "#             age_discretized[i] = age_labels[j]\n",
    "#             break\n",
    "\n",
    "# Compute covariance matrix and its inverse\n",
    "cov_matrix = np.cov(X_num.T)\n",
    "inv_cov_matrix = inv(cov_matrix)\n",
    "\n",
    "# Calculate distances\n",
    "num_dist = vectorized_mahalanobis(X_num, inv_cov_matrix)\n",
    "cat_dist = vectorized_overlap(X_cat)\n",
    "\n",
    "# Normalize distances numerically\n",
    "num_dist = num_dist/np.max(num_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_weights_cd(weights, X, num_dist, cat_dist):\n",
    "    \"\"\"\n",
    "    Optimize the weights for the custom distance matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - weights: List of weights to test\n",
    "    - X: Dataset\n",
    "    - num_dist: Numerical distance matrix\n",
    "    - cat_dist: Categorical distance matrix\n",
    "\n",
    "    Returns:\n",
    "    - best_weights: The best weights\n",
    "    - best_k: The optimal number of clusters\n",
    "    - best_sil: The Silhouette score for the optimal k\n",
    "    \"\"\"\n",
    "\n",
    "    sil_scores = {}\n",
    "    k_best = {}\n",
    "\n",
    "    for i in range(len(weights)):\n",
    "        # test different weights\n",
    "        w_num = weights[i]\n",
    "        w_cat = 1 - w_num\n",
    "        custom_dist = w_num*num_dist + w_cat*cat_dist\n",
    "        print(f'Testing - w_num = {w_num}, w_cat = {w_cat}')\n",
    "        \n",
    "        # check goodness of the weights\n",
    "        k_best[weights[i]], sil_scores[weights[i]] = test_different_cluster_kmedoids(X, custom_dist, [2, 3, 4])\n",
    "\n",
    "    # Get the best weights\n",
    "    best_weights = max(sil_scores, key=sil_scores.get)\n",
    "    print(f'Best weights: {best_weights}')\n",
    "    print(f'Best k: {k_best[best_weights]}')\n",
    "    print(f'Silhouette score: {sil_scores[best_weights]}')\n",
    "\n",
    "    return best_weights, k_best[best_weights], sil_scores[best_weights]\n",
    "\n",
    "best_weights, best_k, best_sil = optimize_weights_cd([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], X, num_dist, cat_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the best distance matrix\n",
    "custom_dist = best_weights*num_dist + (1 - best_weights)*cat_dist\n",
    "\n",
    "# Did K-Medoids clustering using good data achieved\n",
    "kmedoids = KMedoids(n_clusters=best_k, random_state=42)\n",
    "kmedoids.fit(custom_dist)\n",
    "clusters = kmedoids.predict(custom_dist)\n",
    "\n",
    "# Silhoutte score\n",
    "sil_score = silhouette_score(custom_dist, clusters, metric='precomputed')\n",
    "print(f\"Silhouette Score: {sil_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_cluster(X, clusters, custom_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_radar(data):\n",
    "    # Create a copy of the data for normalization\n",
    "    plot_data = data.copy()\n",
    "\n",
    "    # Normalize all numerical variables using MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    plot_data[numerical_features.columns] = scaler.fit_transform(plot_data[numerical_features.columns])\n",
    "\n",
    "    # Calculate mean values for each numerical variable by cluster\n",
    "    cluster_means = plot_data.groupby('Cluster')[numerical_features.columns].mean()\n",
    "\n",
    "    # Set up the radar chart\n",
    "    categories = numerical_features.columns\n",
    "    num_vars = len(categories)\n",
    "    angles = [n / float(num_vars) * 2 * np.pi for n in range(num_vars)]\n",
    "    angles += angles[:1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "    # Plot for each cluster\n",
    "    for cluster in range(data['Cluster'].nunique()):\n",
    "        values = cluster_means.loc[cluster].values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=f'Cluster {cluster}')\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "\n",
    "    # Set chart properties\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.xticks(angles[:-1], categories)\n",
    "    ax.set_title(\"Cluster profiles (all variables are normalized)\", y=1.08)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Radar chart\n",
    "data_with_clusters = data.copy()\n",
    "data_with_clusters['Cluster'] = clusters\n",
    "\n",
    "plot_cluster_radar(data_with_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def plot_cluster_histograms(data_with_clusters):\n",
    "    \"\"\"\n",
    "    Creates separate histogram plots for all variables in the dataset,\n",
    "    colored by cluster assignments.\n",
    "    \n",
    "    Parameters:\n",
    "        data_with_clusters : DataFrame\n",
    "            Dataset containing variables and Cluster assignments\n",
    "            \n",
    "    Returns:\n",
    "        Figure object\n",
    "    \"\"\"\n",
    "    # Define the variables we want to plot\n",
    "    variables = data_with_clusters.columns.values\n",
    "    \n",
    "    # Calculate optimal grid dimensions\n",
    "    n_vars = len(variables)\n",
    "    n_cols = min(4, n_vars)  # Maximum 4 columns\n",
    "    n_rows = math.ceil(n_vars / n_cols)  # Calculate needed rows\n",
    "    \n",
    "    # Create figure with calculated dimensions\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 3*n_rows))\n",
    "    axes = axes.ravel()  # Flatten the array for easier indexing\n",
    "    \n",
    "    # Plot each variable\n",
    "    for var, ax in zip(variables, axes):\n",
    "        sns.histplot(\n",
    "            data=data_with_clusters,\n",
    "            x=var,\n",
    "            hue='Cluster',\n",
    "            kde=True,\n",
    "            stat='density',\n",
    "            common_norm=False,\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.set_title(f'Distribution of {var}')\n",
    "    \n",
    "    # Remove empty subplots if needed\n",
    "    if len(variables) < n_rows * n_cols:\n",
    "        for i in range(len(variables), n_rows * n_cols):\n",
    "            fig.delaxes(axes[i])\n",
    "    \n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Example usage:\n",
    "fig = plot_cluster_histograms(data_with_clusters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
