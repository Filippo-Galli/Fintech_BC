{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup colab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "path = '/content/drive/MyDrive/PoliMI/Dataset1_BankClients.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **IMPORTANT**\n",
    "> Nel caso si utilizzi google Colab c'è da caricare i file di input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path su linux\n",
    "file_path = '../data/Dataset2_Needs.xls'\n",
    "\n",
    "# Path su windows - comando da inserire \n",
    "#path = ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load each sheet into separate DataFrames\n",
    "needs_df = pd.read_excel(file_path, sheet_name='Needs')\n",
    "products_df = pd.read_excel(file_path, sheet_name='Products')\n",
    "metadata_df = pd.read_excel(file_path, sheet_name='Metadata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_variable_summary(df, metadata_df):\n",
    "    # Create empty lists to store the chosen statistics\n",
    "    stats_dict = {\n",
    "        'Variable': [],\n",
    "        'Description': [],\n",
    "        'Mean': [],\n",
    "        'Std': [],\n",
    "        'Missing': [],\n",
    "        'Min': [],\n",
    "        'Max': []\n",
    "    }\n",
    "\n",
    "    # Create a metadata dictionary for easy lookup\n",
    "    meta_dict = dict(zip(metadata_df['Metadata'], metadata_df['Unnamed: 1']))\n",
    "\n",
    "    for col in df.columns:\n",
    "        stats_dict['Variable'].append(col)\n",
    "        stats_dict['Description'].append(meta_dict.get(col, 'N/A'))\n",
    "\n",
    "        # Calculate some statistics for each column\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            stats_dict['Mean'].append(f\"{df[col].mean():.2f}\")\n",
    "            stats_dict['Std'].append(f\"{df[col].std():.2f}\")\n",
    "            stats_dict['Min'].append(f\"{df[col].min():.2f}\")\n",
    "            stats_dict['Max'].append(f\"{df[col].max():.2f}\")\n",
    "        else:\n",
    "            stats_dict['Mean'].append('N/A')\n",
    "            stats_dict['Std'].append('N/A')\n",
    "            stats_dict['Min'].append('N/A')\n",
    "            stats_dict['Max'].append('N/A')\n",
    "\n",
    "        stats_dict['Missing'].append(df[col].isna().sum())\n",
    "\n",
    "    return pd.DataFrame(stats_dict)\n",
    "\n",
    "\n",
    "# Create summary tables\n",
    "print(\"NEEDS VARIABLES SUMMARY:\")\n",
    "needs_summary = create_variable_summary(needs_df, metadata_df)\n",
    "display(needs_summary.style\n",
    "        .set_properties(**{'text-align': 'left'})\n",
    "        .hide(axis='index'))\n",
    "\n",
    "print(\"\\nPRODUCTS VARIABLES SUMMARY:\")\n",
    "products_summary = create_variable_summary(products_df, metadata_df)\n",
    "display(products_summary.style\n",
    "        .set_properties(**{'text-align': 'left'})\n",
    "        .hide(axis='index'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the data for each variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def plot_distribution(df, variable):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df[variable], kde=True)\n",
    "    plt.title(f'Distribution of {variable}')\n",
    "    plt.xlabel(variable)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "# Plot distributions for numeric variables in needs_df\n",
    "numeric_vars = needs_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "for var in numeric_vars:\n",
    "    pass\n",
    "    #plot_distribution(needs_df, var) # Inutile perchè le feature sono tutte bilanciate ed abbastanza distribuite bene a parte l'income e la wealth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Change Wealth in log scale\n",
    "def prepare_features(df):\n",
    "    X = df.copy()\n",
    "\n",
    "    # Log transformation for Wealth and Income\n",
    "    X['Wealth_log'] = np.log1p(X['Wealth'])\n",
    "    X['Income_log'] = np.log1p(X['Income '])\n",
    "\n",
    "    #X['financial_sophistication'] = (X['FinancialEducation'] +  X['RiskPropensity'])*X['Wealth_log']\n",
    "\n",
    "    # Select features for modeling\n",
    "    features_base = ['Age', 'Gender', 'FamilyMembers', 'FinancialEducation',\n",
    "                    'RiskPropensity', 'Wealth_log', 'Income_log']\n",
    "\n",
    "    # Normalize all features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_base = pd.DataFrame(scaler.fit_transform(X[features_base]), columns=features_base)\n",
    "\n",
    "    return X_base\n",
    "\n",
    "# Drop ID column as it's not needed for analysis\n",
    "needs_df = needs_df.drop('ID', axis=1)\n",
    "\n",
    "# Prepare features\n",
    "X_base = prepare_features(needs_df)\n",
    "y_income = needs_df['IncomeInvestment']\n",
    "y_accum = needs_df['AccumulationInvestment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all numeric columns including transformed ones (and not transformed, for comparison)\n",
    "numeric_cols = X_base.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(X_base[numeric_cols].corr(),\n",
    "            annot=True,\n",
    "            cmap='coolwarm',\n",
    "            center=0)\n",
    "plt.title('Correlation matrix of transformed and normalized variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Step 1: Feature engineering and transformation function\n",
    "def prepare_features(df):\n",
    "    X = df.copy()\n",
    "\n",
    "    # Log transformation for Wealth and Income\n",
    "    X['Wealth_log'] = np.log1p(X['Wealth'])\n",
    "    X['Income_log'] = np.log1p(X['Income '])\n",
    "\n",
    "    # Create Income/Wealth ratio\n",
    "    X['Income_Wealth_Ratio'] = X['Income '].div(X['Wealth'].replace(0, np.nan)).fillna(X['Income '].max())\n",
    "    X['Income_Wealth_Ratio_log'] = np.log1p(X['Income_Wealth_Ratio'])\n",
    "\n",
    "    # Select features for modeling\n",
    "    features_base = ['Age', 'Gender', 'FamilyMembers', 'FinancialEducation',\n",
    "                    'RiskPropensity', 'Wealth_log', 'Income_log']\n",
    "\n",
    "    features_engineered = ['Age', 'Gender', 'FamilyMembers', 'FinancialEducation',\n",
    "                          'RiskPropensity', 'Income_Wealth_Ratio_log']\n",
    "\n",
    "    # Normalize all features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_base = pd.DataFrame(scaler.fit_transform(X[features_base]), columns=features_base)\n",
    "    X_engineered = pd.DataFrame(scaler.fit_transform(X[features_engineered]), columns=features_engineered)\n",
    "\n",
    "    return X_base, X_engineered\n",
    "\n",
    "# Step 2: Data split function\n",
    "def split_data(X, y, test_size=0.2, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Step 3: Model training and evaluation function\n",
    "def train_evaluate_model(X_train, y_train, X_test, y_test, model, k_folds=5):\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    cv_metrics = {\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': []\n",
    "    }\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        y_val_pred = model.predict(X_val_fold)\n",
    "\n",
    "        cv_metrics['accuracy'].append(accuracy_score(y_val_fold, y_val_pred))\n",
    "        cv_metrics['precision'].append(precision_score(y_val_fold, y_val_pred))\n",
    "        cv_metrics['recall'].append(recall_score(y_val_fold, y_val_pred))\n",
    "        cv_metrics['f1'].append(f1_score(y_val_fold, y_val_pred))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    return {\n",
    "        'cv_metrics': {\n",
    "            metric: {\n",
    "                'mean': np.mean(scores),\n",
    "                'std': np.std(scores)\n",
    "            } for metric, scores in cv_metrics.items()\n",
    "        },\n",
    "        'test_metrics': {\n",
    "            'accuracy': accuracy_score(y_test, y_test_pred),\n",
    "            'precision': precision_score(y_test, y_test_pred),\n",
    "            'recall': recall_score(y_test, y_test_pred),\n",
    "            'f1': f1_score(y_test, y_test_pred)\n",
    "        }\n",
    "    }, model\n",
    "\n",
    "# Step 4: Display results function\n",
    "def display_results_table(results_dict, model_name, feature_type):\n",
    "    cv_data = {\n",
    "        #'Metric': ['Accuracy', 'Precision', 'Recall', 'F1'],\n",
    "        'Metric': ['Precision', 'F1'],\n",
    "        'CV Mean': [\n",
    "            #results_dict['cv_metrics']['accuracy']['mean'],\n",
    "            results_dict['cv_metrics']['precision']['mean'],\n",
    "            #results_dict['cv_metrics']['recall']['mean'],\n",
    "            results_dict['cv_metrics']['f1']['mean']\n",
    "        ],\n",
    "        'CV Std': [\n",
    "            #results_dict['cv_metrics']['accuracy']['std'],\n",
    "            results_dict['cv_metrics']['precision']['std'],\n",
    "            #results_dict['cv_metrics']['recall']['std'],\n",
    "            results_dict['cv_metrics']['f1']['std']\n",
    "        ],\n",
    "        'Test Set': [\n",
    "            #results_dict['test_metrics']['accuracy'],\n",
    "            results_dict['test_metrics']['precision'],\n",
    "            #results_dict['test_metrics']['recall'],\n",
    "            results_dict['test_metrics']['f1']\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(cv_data)\n",
    "    df = df.round(3)\n",
    "\n",
    "    print(f\"\\n{model_name} - {feature_type}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(tabulate(df, headers='keys', tablefmt='pretty'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    #'SVM': SVC(),\n",
    "    #'NaiveBayes': GaussianNB(), \n",
    "    #'KNN': KNeighborsClassifier(n_neighbors=20),\n",
    "    #'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
    "    #'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    #'BetaBinomial': BernoulliNB(alpha=1)  # Using alpha=1.0 gives a beta(1,1) prior\n",
    "    #'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "    #'GradientBoosting': GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "\n",
    "# Run analysis\n",
    "for target_name, y in [('Income Investment', y_income), ('Accumulation Investment', y_accum)]:\n",
    "    print(f\"\\nTarget Variable: {target_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    X_base_train, X_base_test, y_train, y_test = split_data(X_base, y)\n",
    "    \n",
    "    # Initialize a sub-dictionary for this target\n",
    "    trained_models[target_name] = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        if model_name == 'BetaBinomial':\n",
    "            # Binarize data for the BetaBinomial model\n",
    "            binarizer = Binarizer(threshold=0.5)\n",
    "            X_train_bin = pd.DataFrame(\n",
    "                binarizer.fit_transform(X_base_train), \n",
    "                columns=X_base_train.columns\n",
    "            )\n",
    "            X_test_bin = pd.DataFrame(\n",
    "                binarizer.transform(X_base_test), \n",
    "                columns=X_base_test.columns\n",
    "            )\n",
    "            results_base, trained_model = train_evaluate_model(X_train_bin, y_train, X_test_bin, y_test, model)\n",
    "        else:\n",
    "            results_base, trained_model = train_evaluate_model(X_base_train, y_train, X_base_test, y_test, model)\n",
    "        \n",
    "        # Store the trained model\n",
    "        trained_models[target_name][model_name] = trained_model\n",
    "        \n",
    "        display_results_table(results_base, model_name, 'Base Features')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now let's move to a class of more complex models, a **Multilayer Perceptron (MLP) Neural Network (NN)**, a family of powerful and flexible models.\n",
    "\n",
    "\n",
    "## Multi-Layer Perceptron implementation\n",
    "First we enhanced the MLP with the architecture we describe below and then we also tried to do an hyper-parameter search focusing on the number of layers and on the \n",
    "number of neurons. Finally for the selected model we even perform a k-fold cross validation with 5 folds.\n",
    "\n",
    "### NN architecture rationale\n",
    "* **Decreasing Layer Sizes** (128 → 64 → 32 → 16 → 1): Creates a bottleneck architecture that forces the NN to learn increasingly compact representations (maybe reducing the risk of overfitting).\n",
    "* **Batch Normalization**: Makes training of NN faster and more stable through normalization of the layers' inputs by re-centering and re-scaling. [Batch Normalization](https://arxiv.org/abs/1502.03167) addresses internal covariate shift, particularly important with our mixed-scale financial features.\n",
    "* **Dropout={0.1,0.2}**: [Dropout](https://jmlr.org/papers/v15/srivastava14a.html) is a light regularization that maintains most information while preventing co-adaptation. Basically, dropout has the effect of making the training process noisy, forcing nodes within a layer to probabilistically take on more or less responsibility for the inputs, we alternate it for a less aggressive regularization.\n",
    "* **ReLU**: Provides non-linearity without gradient vanishing issues common in financial data modeling, we also put a sigmoid inside an hidden layer.\n",
    "* **Sigmoid Output**: Transforms final layer output into probability scores for binary classification.\n",
    "\n",
    "### Training configuration and reasoning\n",
    "* **Binary Cross Entropy Loss**: Natural choice for binary classification with probabilistic interpretation\n",
    "* **AdamW Optimizer (lr=0.001)**: Adaptive learning rates help handle different feature scales, to which we will add a scheduler for better performance.\n",
    "* **Batch size=128**: Good balance between computational efficiency and gradient estimation.\n",
    "* **500 epochs**: Sufficient iterations for convergence with validation monitoring every 50 epochs.\n",
    "\n",
    "### Data handling\n",
    "* Custom PyTorch Dataset class for efficient data loading.\n",
    "* Separate training and evaluation pipelines.\n",
    "* Comprehensive metrics evaluation (accuracy, precision, recall, F1), comparable to previous models.\n",
    "\n",
    "### Key Enhancements\n",
    "* **Learning rate scheduler**: Implements **ReduceLROnPlateau** for **adaptive learning rate adjustment** - it's a smart technique, look at [this paper](https://arxiv.org/abs/1506.01186) and [this other one](https://arxiv.org/abs/1803.09820):\n",
    "  * Reduces learning rate by 50% after 30 epochs without improvement.\n",
    "  * Helps overcome plateaus and fine-tune learning in later stages.\n",
    "\n",
    "* **Performance monitoring**:\n",
    "  * Tracks training and validation losses across epochs.\n",
    "  * Monitors validation accuracy progression.\n",
    "  * Stores metrics for visualization.\n",
    "\n",
    "* **Visualization capabilities**:\n",
    "  * Real-time learning curves display.\n",
    "  * Side-by-side loss and accuracy plots.\n",
    "  * Visual assessment of model convergence and potential overfitting. Overfitting occurs when a model **performs well on training data but poorly on validation data**. In learning curves, this appears as:  \n",
    "    * **Training loss decreasing steadily**.\n",
    "    * **validation loss stops improving (plateaus) or increases** after a certain epoch.\n",
    "    * A **large gap** between training and validation performance indicates overfitting, suggesting the need for **regularization, more data, or early stopping**.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Custom dataset class\n",
    "class InvestmentDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X.values)\n",
    "        self.y = torch.FloatTensor(y.values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Enhanced MLP class, 1 more layer\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            # New initial 128 neurons layer\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),  # Dropout 0.2\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),  # Prima ReLU\n",
    "            nn.Dropout(0.1),  # Alternate  0.1\n",
    "            \n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),  \n",
    "            nn.Dropout(0.2),  # Back to 0.2\n",
    "            \n",
    "            nn.Linear(32, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),  # Terza ReLU\n",
    "            nn.Dropout(0.1),  # Alternato a 0.1\n",
    "            \n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()  # Output activation probability: Sigmoid \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Classe per l'Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=50, min_delta=0, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_model = None\n",
    "    \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = model.state_dict()\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "\n",
    "# Training with learning rate scheduler and early stopping\n",
    "def train_model_with_curves(model, train_loader, val_loader, criterion, optimizer, epochs=500, patience=50):\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=30)\n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch).squeeze()\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                y_pred = model(X_batch).squeeze()\n",
    "                val_loss += criterion(y_pred, y_batch).item()\n",
    "                val_preds.extend((y_pred > 0.5).float().numpy())\n",
    "                val_true.extend(y_batch.numpy())\n",
    "\n",
    "        # Calcola metriche medie\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = accuracy_score(val_true, val_preds)\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        early_stopping(avg_val_loss, model)\n",
    "\n",
    "        if (epoch + 1) % 50 == 0 or early_stopping.early_stop:\n",
    "            print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, '\n",
    "                  f'Val Loss: {avg_val_loss:.4f}, '\n",
    "                  f'Val Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(f'Early stopping triggered after {epoch + 1} epochs')\n",
    "            # Load best weights\n",
    "            model.load_state_dict(early_stopping.best_model_state)\n",
    "            break\n",
    "\n",
    "    # Plot learning curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Model evaluation\n",
    "def evaluate_nn_metrics(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            y_pred = model(X_batch).squeeze()\n",
    "            all_preds.extend((y_pred > 0.5).float().numpy())\n",
    "            all_true.extend(y_batch.numpy())\n",
    "\n",
    "    return {\n",
    "        #'accuracy': accuracy_score(all_true, all_preds),\n",
    "        'precision': precision_score(all_true, all_preds),\n",
    "        #'recall': recall_score(all_true, all_preds),\n",
    "        'f1': f1_score(all_true, all_preds)\n",
    "    }\n",
    "\n",
    "# Training and evaluation for both targets\n",
    "for target_name, y in [('Income Investment', y_income), ('Accumulation Investment', y_accum)]:\n",
    "    print(f\"\\nTraining Neural Network for {target_name}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = split_data(X_base, y)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataset = InvestmentDataset(X_train, y_train)\n",
    "    test_dataset = InvestmentDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "    # Initialize model, criterion, optimizer\n",
    "    model = MLP(input_size=X_train.shape[1])\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train model with early stopping\n",
    "    train_model_with_curves(model, train_loader, test_loader, criterion, optimizer, patience=50)\n",
    "\n",
    "    # Display final metrics\n",
    "    print(f\"\\nResults for {target_name}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    train_metrics = evaluate_nn_metrics(model, train_loader)\n",
    "    test_metrics = evaluate_nn_metrics(model, test_loader)\n",
    "\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Training': train_metrics,\n",
    "        'Test': test_metrics\n",
    "    }).round(3)\n",
    "\n",
    "    print(\"\\nNeural Network Metrics:\")\n",
    "    print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvestmentDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X.values)\n",
    "        self.y = torch.FloatTensor(y.values)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, n_layers=3, neurons=(128, 64, 32)):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        # Crea le layer intermedie\n",
    "        for i, neurons_count in enumerate(neurons[:n_layers]):\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, neurons_count),\n",
    "                nn.BatchNorm1d(neurons_count),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2 if i % 2 == 0 else 0.1)\n",
    "            ])\n",
    "            prev_size = neurons_count\n",
    "            \n",
    "        # Aggiungi l'ultimo layer di output\n",
    "        layers.extend([\n",
    "            nn.Linear(prev_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        ])\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=50, min_delta=0, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "    \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = model.state_dict()\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "\n",
    "def train_model_with_curves(model, train_loader, val_loader, criterion, optimizer, epochs=500, patience=50):\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=30)\n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch).squeeze()\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                y_pred = model(X_batch).squeeze()\n",
    "                val_loss += criterion(y_pred, y_batch).item()\n",
    "                val_preds.extend((y_pred > 0.5).float().numpy())\n",
    "                val_true.extend(y_batch.numpy())\n",
    "        \n",
    "        # Calcola metriche medie\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = accuracy_score(val_true, val_preds)\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        early_stopping(avg_val_loss, model)\n",
    "        if (epoch + 1) % 50 == 0 or early_stopping.early_stop:\n",
    "            print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, '\n",
    "                  f'Val Loss: {avg_val_loss:.4f}, '\n",
    "                  f'Val Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(f'Early stopping triggered after {epoch + 1} epochs')\n",
    "            break\n",
    "            \n",
    "        # Load best weights\n",
    "        model.load_state_dict(early_stopping.best_model_state)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def hyperparameter_search(X_train, y_train, X_test, y_test):\n",
    "    # Definisci le configurazioni da provare\n",
    "    layer_configs = [3, 4]\n",
    "    neurons_config = (126, 64, 32)\n",
    "    \n",
    "    best_results = {}\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for n_layers in layer_configs:\n",
    "        print(f\"\\nProvo configurazione con {n_layers} layers\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        fold_results = []\n",
    "        \n",
    "        # K-fold cross validation\n",
    "        for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train)):\n",
    "            print(f\"\\nFold {fold + 1}/5\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            # Crea dataset per questo fold\n",
    "            X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "            y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "            \n",
    "            # Crea i dataloader\n",
    "            train_dataset = InvestmentDataset(X_fold_train, y_fold_train)\n",
    "            val_dataset = InvestmentDataset(X_fold_val, y_fold_val)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "            \n",
    "            # Inizializza modello e ottimizzatore\n",
    "            model = MLP(input_size=X_train.shape[1], n_layers=n_layers, neurons=neurons_config)\n",
    "            criterion = nn.BCELoss()\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "            \n",
    "            # Train model\n",
    "            trained_model = train_model_with_curves(\n",
    "                model, \n",
    "                train_loader, \n",
    "                val_loader, \n",
    "                criterion, \n",
    "                optimizer,\n",
    "                patience=50\n",
    "            )\n",
    "            \n",
    "            # Valuta il modello sul test set di questo fold\n",
    "            metrics = evaluate_nn_metrics(trained_model, val_loader)\n",
    "            fold_results.append(metrics['f1'])\n",
    "        \n",
    "        # Calcola media delle metriche su tutti i folds\n",
    "        avg_f1 = np.mean(fold_results)\n",
    "        best_results[n_layers] = {\n",
    "            'avg_f1': avg_f1,\n",
    "            'fold_results': fold_results\n",
    "        }\n",
    "    \n",
    "    return best_results\n",
    "\n",
    "def evaluate_nn_metrics(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            y_pred = model(X_batch).squeeze()\n",
    "            all_preds.extend((y_pred > 0.5).float().numpy())\n",
    "            all_true.extend(y_batch.numpy())\n",
    "    \n",
    "    return {\n",
    "        #'accuracy': accuracy_score(all_true, all_preds),\n",
    "        'precision': precision_score(all_true, all_preds),\n",
    "        #'recall': recall_score(all_true, all_preds),\n",
    "        'f1': f1_score(all_true, all_preds)\n",
    "    }\n",
    "\n",
    "# Training loop principale\n",
    "for target_name, y in [('Income Investment', y_income), ('Accumulation Investment', y_accum)]:\n",
    "    print(f\"\\nTraining Neural Network for {target_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = split_data(X_base, y)\n",
    "    \n",
    "    # Esegui ricerca iperparametri\n",
    "    results = hyperparameter_search(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Trova la migliore configurazione\n",
    "    best_n_layers = max(results.items(), key=lambda x: x[1]['avg_f1'])[0]\n",
    "    print(f\"\\nBest configuration found:\")\n",
    "    print(f\"Number of layers: {best_n_layers}\")\n",
    "    print(f\"Average F1-score across folds: {results[best_n_layers]['avg_f1']:.4f}\")\n",
    "    print(\"\\nResults for each fold:\")\n",
    "    for i, score in enumerate(results[best_n_layers]['fold_results']):\n",
    "        print(f\"Fold {i+1}: F1-score = {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation System "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target and model selection\n",
    "target_name = 'Accumulation Investment'\n",
    "model_name = 'RandomForest'\n",
    "y_accum = needs_df['AccumulationInvestment']\n",
    "X_train, X_test, y_train, y_test = split_data(X_base, y_accum)\n",
    "\n",
    "# Prepare Features\n",
    "y_pred = trained_models[target_name][model_name].predict(X_test)\n",
    "\n",
    "# Filtering accumulation products from our product database\n",
    "accumulation_products = products_df[products_df['Type'] == 1].copy()\n",
    "min_risk = accumulation_products['Risk'].min()\n",
    "\n",
    "# Collecting client IDs and their risk propensities from the test set\n",
    "# We filter for clients predicted to need accumulation products (y_pred == 1)\n",
    "client_indices = np.where(y_pred == 1)[0]\n",
    "target_client_ids = needs_df.iloc[X_test.index[client_indices]].index.values\n",
    "\n",
    "# Get client information and create additional features\n",
    "client_data = X_test.iloc[client_indices].copy()\n",
    "client_risk_propensity = client_data['RiskPropensity'].values\n",
    "client_age = client_data['Age'].values\n",
    "\n",
    "# Apply age-based risk adjustment - younger clients can typically handle more risk\n",
    "age_adjustment_factor = 1 + (0.1 * (1 - client_data['Age']))\n",
    "adjusted_risk_propensity = client_risk_propensity * age_adjustment_factor.values\n",
    "\n",
    "# Risk buffer - slightly increase tolerance to find more matches\n",
    "risk_buffer = 0.05\n",
    "\n",
    "# Extract product information for matching\n",
    "product_ids = accumulation_products['IDProduct'].astype(np.uint16).values\n",
    "product_risks = accumulation_products['Risk'].values\n",
    "\n",
    "# Initialize lists to store recommendation results\n",
    "primary_recommendations = []\n",
    "primary_risk_levels = []\n",
    "alternative_recommendations = []\n",
    "alternative_risk_levels = []\n",
    "recommendation_confidence = []\n",
    "\n",
    "# Generate personalized recommendations for each target client\n",
    "for i in range(len(client_risk_propensity)):\n",
    "    original_risk = client_risk_propensity[i]\n",
    "    adjusted_risk = adjusted_risk_propensity[i]\n",
    "    \n",
    "    # IMPROVEMENT 1: Use adjusted risk with buffer and allow equal risk levels\n",
    "    suitable_products_mask = product_risks <= (adjusted_risk + risk_buffer)\n",
    "    suitable_products = product_risks[suitable_products_mask]\n",
    "    suitable_product_ids = product_ids[suitable_products_mask]\n",
    "    \n",
    "    if len(suitable_products) > 0:\n",
    "        # IMPROVEMENT 2: Find the product with risk closest to client's risk propensity\n",
    "        # This gives the best risk-return match while respecting tolerance\n",
    "        risk_differences = np.abs(suitable_products - original_risk)\n",
    "        best_match_idx = np.argmin(risk_differences)\n",
    "        \n",
    "        recommended_product_id = suitable_product_ids[best_match_idx]\n",
    "        recommended_risk = suitable_products[best_match_idx]\n",
    "        \n",
    "        # Calculate confidence score based on how close the match is\n",
    "        max_possible_diff = max(product_risks.max() - product_risks.min(), 1)\n",
    "        match_confidence = 1 - (risk_differences[best_match_idx] / max_possible_diff)\n",
    "        \n",
    "        primary_recommendations.append(recommended_product_id)\n",
    "        primary_risk_levels.append(recommended_risk)\n",
    "        recommendation_confidence.append(match_confidence)\n",
    "        \n",
    "        # IMPROVEMENT 3: Find an alternative recommendation (second best match)\n",
    "        if len(suitable_products) > 1:\n",
    "            # Create a mask to exclude the primary recommendation\n",
    "            alt_mask = np.ones_like(suitable_products, dtype=bool)\n",
    "            alt_mask[best_match_idx] = False\n",
    "            \n",
    "            alt_products = suitable_products[alt_mask]\n",
    "            alt_product_ids = suitable_product_ids[alt_mask]\n",
    "            \n",
    "            # Find second best match\n",
    "            alt_differences = np.abs(alt_products - original_risk)\n",
    "            second_best_idx = np.argmin(alt_differences)\n",
    "            \n",
    "            alternative_recommendations.append(alt_product_ids[second_best_idx])\n",
    "            alternative_risk_levels.append(alt_products[second_best_idx])\n",
    "        else:\n",
    "            alternative_recommendations.append(0)\n",
    "            alternative_risk_levels.append(0)\n",
    "    else:\n",
    "        # IMPROVEMENT 4: Use fallback strategy for clients with no exact matches\n",
    "        # Find the product with lowest risk as a conservative recommendation\n",
    "        if original_risk >= (min_risk - risk_buffer):\n",
    "            lowest_risk_idx = np.argmin(product_risks)\n",
    "            fallback_product_id = product_ids[lowest_risk_idx]\n",
    "            fallback_risk = product_risks[lowest_risk_idx]\n",
    "            \n",
    "            primary_recommendations.append(fallback_product_id)\n",
    "            primary_risk_levels.append(fallback_risk)\n",
    "            # Lower confidence for fallback recommendations\n",
    "            recommendation_confidence.append(0.5)\n",
    "            \n",
    "            alternative_recommendations.append(0)\n",
    "            alternative_risk_levels.append(0)\n",
    "        else:\n",
    "            # No suitable products even with fallback strategy\n",
    "            primary_recommendations.append(0)\n",
    "            primary_risk_levels.append(0)\n",
    "            recommendation_confidence.append(0)\n",
    "            \n",
    "            alternative_recommendations.append(0)\n",
    "            alternative_risk_levels.append(0)\n",
    "\n",
    "# Create an enhanced recommendation matrix with multiple recommendations and confidence scores\n",
    "enhanced_nba = pd.DataFrame({\n",
    "    'ClientID': target_client_ids,\n",
    "    'OriginalRiskPropensity': client_risk_propensity,\n",
    "    'AdjustedRiskPropensity': adjusted_risk_propensity,\n",
    "    'Age': client_age,\n",
    "    'PrimaryProductID': primary_recommendations,\n",
    "    'PrimaryProductRisk': primary_risk_levels,\n",
    "    'AlternativeProductID': alternative_recommendations,\n",
    "    'AlternativeProductRisk': alternative_risk_levels,\n",
    "    'ConfidenceScore': recommendation_confidence\n",
    "})\n",
    "\n",
    "# Add product names for better interpretability\n",
    "product_name_dict = dict(zip(\n",
    "    accumulation_products['IDProduct'], \n",
    "    accumulation_products['IDProduct'].astype(str) + ': Risk=' + accumulation_products['Risk'].astype(str)\n",
    "))\n",
    "product_name_dict[0] = 'No Recommendation'\n",
    "\n",
    "enhanced_nba['PrimaryProductName'] = enhanced_nba['PrimaryProductID'].map(product_name_dict)\n",
    "enhanced_nba['AlternativeProductName'] = enhanced_nba['AlternativeProductID'].map(product_name_dict)\n",
    "\n",
    "# Calculate recommendation statistics\n",
    "total_clients = len(enhanced_nba)\n",
    "clients_with_recommendations = len(enhanced_nba[enhanced_nba['PrimaryProductID'] > 0])\n",
    "clients_with_alternatives = len(enhanced_nba[(enhanced_nba['PrimaryProductID'] > 0) & \n",
    "                                           (enhanced_nba['AlternativeProductID'] > 0)])\n",
    "percentage_with_recommendations = (clients_with_recommendations / total_clients) * 100\n",
    "\n",
    "print(\"\\nEnhanced Recommendation Statistics:\")\n",
    "print(f\"Total customers analyzed: {total_clients}\")\n",
    "print(f\"Customers with primary recommendations: {clients_with_recommendations} ({percentage_with_recommendations:.2f}%)\")\n",
    "print(f\"Customers with alternative recommendations: {clients_with_alternatives}\")\n",
    "print(f\"Customers without suitable recommendations: {total_clients - clients_with_recommendations}\")\n",
    "\n",
    "# Display sample recommendations\n",
    "print(\"\\nTop 5 personalized recommendations:\")\n",
    "display_cols = ['ClientID', 'OriginalRiskPropensity', 'PrimaryProductID', \n",
    "                'PrimaryProductRisk', 'AlternativeProductID', 'ConfidenceScore']\n",
    "print(enhanced_nba[display_cols].head(5))\n",
    "\n",
    "# Enhanced visualization: Suitability Chart with improved styling and information\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot points with color based on confidence score\n",
    "scatter = plt.scatter(\n",
    "    enhanced_nba['OriginalRiskPropensity'],\n",
    "    enhanced_nba['PrimaryProductRisk'],\n",
    "    c=enhanced_nba['ConfidenceScore'],\n",
    "    cmap='viridis',\n",
    "    alpha=0.8,\n",
    "    s=100,\n",
    "    edgecolors='white',\n",
    "    linewidths=0.5\n",
    ")\n",
    "\n",
    "# Add reference line for perfect risk matching\n",
    "max_val = max(\n",
    "    enhanced_nba['OriginalRiskPropensity'].max(),\n",
    "    enhanced_nba['PrimaryProductRisk'].max(),\n",
    "    enhanced_nba['AdjustedRiskPropensity'].max()\n",
    ")\n",
    "plt.plot([0, max_val], [0, max_val], 'r--', alpha=0.7, label='Ideal Risk Match')\n",
    "\n",
    "# Add buffer zone visualization\n",
    "x = np.linspace(0, max_val, 100)\n",
    "plt.fill_between(x, x, x + risk_buffer, alpha=0.2, color='green', label=f'Risk Buffer Zone (+{risk_buffer})')\n",
    "\n",
    "# Formatting\n",
    "plt.colorbar(scatter, label='Recommendation Confidence')\n",
    "plt.title('Enhanced Suitability Analysis: Client Risk vs Product Risk', fontsize=14)\n",
    "plt.xlabel('Client Risk Propensity', fontsize=12)\n",
    "plt.ylabel('Recommended Product Risk Level', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot additional information - Risk adjustment visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    enhanced_nba['OriginalRiskPropensity'], \n",
    "    enhanced_nba['AdjustedRiskPropensity'],\n",
    "    c=enhanced_nba['Age'],\n",
    "    cmap='coolwarm',\n",
    "    alpha=0.8,\n",
    "    s=80\n",
    ")\n",
    "plt.colorbar(label='Client Age')\n",
    "plt.title('Risk Propensity Adjustment by Age', fontsize=14)\n",
    "plt.xlabel('Original Risk Propensity', fontsize=12)\n",
    "plt.ylabel('Age-Adjusted Risk Propensity', fontsize=12)\n",
    "plt.plot([0, max_val], [0, max_val], 'k--', alpha=0.5)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Product recommendation distribution\n",
    "if clients_with_recommendations > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Count products and sort by frequency\n",
    "    recommendation_counts = enhanced_nba['PrimaryProductID'].value_counts().sort_values(ascending=False)\n",
    "    recommendation_counts = recommendation_counts[recommendation_counts.index > 0]  # Exclude ID 0\n",
    "    \n",
    "    # Plot top 10 recommendations (or all if less than 10)\n",
    "    top_n = min(10, len(recommendation_counts))\n",
    "    top_products = recommendation_counts.head(top_n)\n",
    "    \n",
    "    bars = plt.bar(\n",
    "        [product_name_dict[pid] for pid in top_products.index], \n",
    "        top_products.values, \n",
    "        color='skyblue'\n",
    "    )\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width()/2.,\n",
    "            height + 0.5,\n",
    "            f'{height/clients_with_recommendations*100:.1f}%',\n",
    "            ha='center', \n",
    "            va='bottom',\n",
    "            rotation=0\n",
    "        )\n",
    "    \n",
    "    plt.title('Top Product Recommendations Distribution', fontsize=14)\n",
    "    plt.xlabel('Product ID and Risk Level', fontsize=12)\n",
    "    plt.ylabel('Number of Recommendations', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
